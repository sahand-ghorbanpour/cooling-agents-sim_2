x-health: &health
  interval: 10s
  timeout: 5s
  retries: 30
  start_period: 30s

networks:
  core:
    driver: bridge
  ai:
    driver: bridge
  sensors:
    driver: bridge
  simulation:
    driver: bridge

volumes:
  nats-data:
  redis-data:
  models-cache:
  nim_cache:
  influx-data:
  grafana-data:
  simulation-data:
  loki-data:
  tempo-data:

services:
  # ========== Message Bus & Cache ==========
  nats:
    image: nats:2.10.14
    command: ["-js", "-sd", "/data", "-m", "8222"]
    ports: ["4222:4222", "8222:8222"]
    volumes: ["nats-data:/data"]
    networks: [core]
    # healthcheck:
    #   # Simplest possible check - if container is running, it's healthy
    #   test: ["CMD-SHELL", "test -f /proc/1/stat"]
    #   interval: 10s
    #   timeout: 5s
    #   retries: 30
    #   start_period: 30s

  nats-exporter:
    image: natsio/prometheus-nats-exporter:latest
    command: ["-varz", "-connz", "-routez", "-subz", "http://nats:8222"]
    networks: [core]
    depends_on: [nats]

  redis:
    image: redis:7
    command: ["redis-server", "--save", "60", "1000", "--appendonly", "yes"]
    ports: ["6379:6379"]
    volumes: ["redis-data:/data"]
    networks: [core]
    healthcheck:
      # Use redis-cli which is available in the redis image
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s

  redis-exporter:
    image: oliver006/redis_exporter:latest
    command: ["--redis.addr=redis://redis:6379"]
    networks: [core]
    depends_on: [redis]

  # ========== LLM Services ==========
  nim-llm:
    image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.2.2
    profiles: ["nim"]
    gpus: "all"
    shm_size: "32gb"
    restart: unless-stopped
    ulimits:
      memlock: -1
      stack: 67108864
    cap_add: ["IPC_LOCK"]
    environment:
      NGC_API_KEY: ${NGC_API_KEY}
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-0,1}
      NIM_CACHE_PATH: /tmp/nim_cache
      TRANSFORMERS_CACHE: /tmp/nim_cache/huggingface/hub
      HF_HOME: /tmp/nim_cache/huggingface
      HUGGINGFACE_HUB_CACHE: /tmp/nim_cache/huggingface/hub
      # NCCL settings for multi-GPU
      NCCL_DEBUG: WARN
      NCCL_IB_DISABLE: "1"
      NCCL_SOCKET_IFNAME: "^docker,lo"
      NCCL_P2P_DISABLE: "1"
      NCCL_SHM_DISABLE: "1"
      NCCL_NET_GDR_LEVEL: "0"
      NCCL_TREE_THRESHOLD: "0"
      NCCL_ALGO: "Ring"
      NCCL_IGNORE_CPU_AFFINITY: "1"
      CUDA_DEVICE_ORDER: "PCI_BUS_ID"
      CUDA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-0,1}
      OMP_NUM_THREADS: "8"
      MKL_NUM_THREADS: "8"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    runtime: nvidia
    ports: ["8000:8000"]
    networks: [ai]
    volumes:
      - ./nim_cache:/tmp/nim_cache
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(5); s.connect(('127.0.0.1',8000)); print('OK')\""]
      <<: *health

  vllm:
    image: vllm/vllm-openai:latest
    profiles: ["vllm"]
    command: >
      --model ${VLLM_MODEL:-meta-llama/Llama-3.1-8B-Instruct}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-8192}
      --dtype auto
      --api-key dummy
    environment:
      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    runtime: nvidia
    volumes:
      - models-cache:/root/.cache/huggingface
    ports: ["8001:8000"]
    networks: [ai]
    healthcheck:
      test: ["CMD-SHELL", "python3 - <<'PY'\nimport socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',8000)); print('OK')\nPY"]
      interval: 10s
      timeout: 3s
      retries: 10

  llm-gateway:
    build:
      context: ./services/llm-gateway
      dockerfile: Dockerfile
    environment:
      METRICS_PORT: 9002
      OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo:4317
      NIM_URL: http://nim-llm:8000
      OPENAI_BASE_URL: http://nim-llm:8000
      NIM_MODEL: ${NIM_MODEL:-meta/llama-3.1-8b-instruct}
      OPENAI_API_KEY: ${NGC_API_KEY}
      REDIS_URL: redis://redis:6379
    ports: ["9000:9000"]
    networks: [ai, core]
    volumes:
      - ./common:/app/common:ro
    depends_on:
      nim-llm:
        condition: service_healthy
      tempo:
        condition: service_started
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',9000)); print('OK')\""]
      <<: *health

  # ========== AI Services ==========
  guardrails:
    build:
      context: ./services/guardrails
      dockerfile: Dockerfile
    environment:
      METRICS_PORT: 9003
      OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo:4317
    networks: [ai, core]
    depends_on:
      tempo:
        condition: service_started
    volumes:
      - ./common:/app/common:ro
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',8001)); print('OK')\""]
      <<: *health

  mcp-math:
    build:
      context: ./services/mcp-math
      dockerfile: Dockerfile
    environment:
      METRICS_PORT: 9004
      OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo:4317
      TARGET_TEMP_K: ${TARGET_TEMP_K:-303.15}
      TEMP_TOLERANCE: ${TEMP_TOLERANCE:-0.5}
    ports: ["7000:7000"]
    networks: [core]
    depends_on:
      tempo:
        condition: service_started
    volumes:
      - ./common:/app/common:ro
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',7000)); print('OK')\""]
      <<: *health

  # ========== Sensor Agent Service (Separate from LangGraph) ==========
  sensor-agent:
    build:
      context: ./services/sensor-agent
      dockerfile: Dockerfile
    environment:
      NATS_URL: nats://nats:4222
      METRICS_PORT: 9012
      OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo:4317
      # FMU-specific environment variables
      FMU_PATH: /app/env/LC_Frontier_5Cabinet_4_17_25.fmu
      EXOGENOUS_DATA_PATH: /app/env/input_04-07-24.csv
      # Simulation parameters
      AUTONOMOUS_SIMULATION_INTERVAL_MS: 5000
      SIMULATION_START_TIME: 0
      SIMULATION_STOP_TIME: 86400
      SIMULATION_STEP_SIZE: 15.0
    ports: ["9012:9012"]
    networks: [core, simulation]
    volumes:
      - ./env:/app/env:ro
      - ./simulation:/app/simulation:ro
      - ./common:/app/common:ro 
      - simulation-data:/app/simulation_data
    depends_on:
      nats:
        condition: service_started
      tempo:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',9012)); print('OK')\""]
      <<: *health

  # ========== LangGraph Studio (Modified) ==========
  langgraph-studio:
    image: python:3.11
    working_dir: /app
    volumes: ["./:/app"]
    command: bash -c "pip install -r requirements.txt && python -c 'from common.metrics import init_metrics; init_metrics(9001)' && langgraph dev --config langgraph.json --host 0.0.0.0 --port 2024"
    environment:
      METRICS_PORT: 9001
      OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo:4317
      NATS_URL: nats://nats:4222
      REDIS_URL: redis://redis:6379
      # LangSmith Integration (IMPORTANT - Keep these!)
      LANGCHAIN_TRACING_V2: ${LANGCHAIN_TRACING_V2:-false}
      LANGCHAIN_API_KEY: ${LANGSMITH_API_KEY:-}
      LANGCHAIN_PROJECT: ${LANGSMITH_PROJECT:-Cooling-DigitalTwin}
      # Additional environment variables
      TARGET_TEMP_C: ${TARGET_TEMP_C:-24.0}
      NIM_MODEL: ${NIM_MODEL:-meta/llama-3.1-8b-instruct}
      # LangGraph will coordinate, but NOT run the simulation directly
      SENSOR_AGENT_MODE: "external"
    ports: ["2024:2024"]
    networks: [core]
    depends_on:
      nats:
        condition: service_started
      redis:
        condition: service_healthy
      sensor-agent:
        condition: service_healthy
      tempo:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',2024)); print('OK')\""]
      <<: *health

  # ========== Agent Services ==========
  nats-bridge:
    build:
      context: ./bridges
      dockerfile: Dockerfile
    environment:
      BRIDGE_METRICS_PORT: 9005
      NATS_URL: nats://nats:4222
    networks: [core]
    depends_on:
      nats:
        condition: service_started
      langgraph-studio:
        condition: service_healthy
    volumes:
      - ./common:/app/common:ro
      - ./graphs:/app/graphs:ro
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',9005)); print('OK')\""]
      <<: *health

  scheduler:
    build:
      context: ./bridges
      dockerfile: Dockerfile
    environment:
      SCHED_METRICS_PORT: 9006
      NATS_URL: nats://nats:4222
      REDIS_URL: redis://redis:6379
    networks: [core]
    depends_on:
      nats:
        condition: service_started
      redis:
        condition: service_healthy
    volumes:
      - ./common:/app/common:ro
    command: ["python", "bridges/scheduler.py"]
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',9006)); print('OK')\""]
      <<: *health

  # ========== Control Services ==========
  control-runtime:
    build:
      context: ./services/control_runtime
      dockerfile: Dockerfile
    environment:
      METRICS_PORT: 9010
      NATS_URL: nats://nats:4222
      REDIS_URL: redis://redis:6379
      TARGET_TEMP_C: ${TARGET_TEMP_C:-24.0}
      LLM_GATEWAY_URL: http://llm-gateway:9000
      MAX_RETRIES: 3
      FALLBACK_ENABLED: true
    networks: [core]
    ports: []
    volumes:
      - ./common:/app/common:ro
    depends_on:
      nats:
        condition: service_started
      redis:
        condition: service_healthy
      llm-gateway:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',9010)); print('OK')\""]
      <<: *health

  sensor-gateway:
    build:
      context: ./services/sensor-gateway
      dockerfile: Dockerfile
    environment:
      METRICS_PORT: 9011
      SENSOR_GATEWAY_PORT: 9011
      NATS_URL: nats://nats:4222
      REDIS_URL: redis://redis:6379
    ports: ["9011:9011"]
    networks: [core, sensors]
    volumes:
      - ./common:/app/common:ro
    depends_on:
      nats:
        condition: service_started
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',9011)); print('OK')\""]
      <<: *health

  # ========== Observability Stack ==========
  prometheus:
    image: prom/prometheus:latest
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=7d'
      - '--web.enable-lifecycle'
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports: ["9090:9090"]
    networks: [core]

  loki:
    image: grafana/loki:2.9.0
    command: -config.file=/etc/loki/loki-config.yml
    volumes:
      - ./observability/loki-config.yml:/etc/loki/loki-config.yml:ro
      - loki-data:/loki
    ports: ["3100:3100"]
    networks: [core]

  tempo:
    image: grafana/tempo:2.5.0
    command: ["-config.file=/etc/tempo/tempo-config.yml"]
    volumes:
      - ./observability/tempo-config.yml:/etc/tempo/tempo-config.yml:ro
    ports: ["3200:3200", "4317:4317"]
    networks: [core]

  grafana:
    image: grafana/grafana:10.4.2
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - ./observability/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana-data:/var/lib/grafana
    ports: ["3000:3000"]
    networks: [core]
    depends_on:
      - prometheus
      - loki
      - tempo

  # ========== Time Series Database ==========
  influxdb:
    image: influxdb:2.7
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUX_PASSWORD:-admin123}
      - DOCKER_INFLUXDB_INIT_ORG=cooling-system
      - DOCKER_INFLUXDB_INIT_BUCKET=telemetry
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=${INFLUX_TOKEN:-dev-token-change-in-production}
    ports: ["8086:8086"]
    networks: [core]
    volumes:
      - influx-data:/var/lib/influxdb2

  influx-writer:
    image: python:3.11
    working_dir: /app
    volumes: ["./:/app"]
    command: bash -c "pip install -r requirements.txt && python telemetry/influx_writer.py"
    environment:
      INFLUX_URL: http://influxdb:8086
      INFLUX_TOKEN: ${INFLUX_TOKEN:-dev-token-change-in-production}
      INFLUX_ORG: cooling-system
      INFLUX_BUCKET: telemetry
      NATS_URL: nats://nats:4222
      METRICS_PORT: 9007
    networks: [core]
    depends_on:
      nats:
        condition: service_started
      influxdb:
        condition: service_started

  # ========== Development Tools ==========
  # Jupyter notebook for analysis (optional)
  jupyter:
    image: jupyter/scipy-notebook:latest
    profiles: ["development"]
    ports: ["8888:8888"]
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      JUPYTER_TOKEN: ${JUPYTER_TOKEN:-cooling-analysis}
    volumes:
      - "./notebooks:/home/jovyan/work"
      - "./data:/home/jovyan/data"
    networks: [core]

  # Redis Commander for Redis inspection (optional)
  redis-commander:
    image: rediscommander/redis-commander:latest
    profiles: ["development"] 
    environment:
      REDIS_HOSTS: local:redis:6379
    ports: ["8081:8081"]
    networks: [core]
    depends_on: [redis]

  # NATS monitoring (optional)
  nats-surveyor:
    image: natsio/nats-surveyor:latest
    profiles: ["development"]
    command: ["--server", "nats://nats:4222", "--port", "7777"]
    ports: ["7777:7777"]
    networks: [core]
    depends_on: [nats]

# ========== Volume Definitions ==========
# All volumes are defined above to ensure data persistence